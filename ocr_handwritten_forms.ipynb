{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR for Handwritten Forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select image to extract text from\n",
    "original_image = 'images/image_2.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OpenCV library\n",
    "import cv2\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(original_image)\n",
    "\n",
    "# Convert to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply adaptive thresholding\n",
    "thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                               cv2.THRESH_BINARY, 11, 2)\n",
    "\n",
    "# Save the processed image (optional)\n",
    "processed_image = 'processed_images/processed_image_1.jpg'\n",
    "cv2.imwrite(processed_image, thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Text using EasyOCR library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import EasyOCR library\n",
    "import easyocr\n",
    "\n",
    "# Initialize reader object\n",
    "reader = easyocr.Reader(['en'])\n",
    "\n",
    "# Extract text from original image\n",
    "result = reader.readtext(original_image, detail=0)\n",
    "\n",
    "# Join the extracted text into a single string\n",
    "extracted_text = '\\n'.join(result)\n",
    "\n",
    "# Check output\n",
    "print(extracted_text)\n",
    "\n",
    "# Save the extracted text to a file\n",
    "output_text = 'output_text/easyocr_output_text_1.txt'\n",
    "with open(output_text, 'w') as file:\n",
    "    file.write(extracted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SpaCy library\n",
    "import spacy\n",
    "\n",
    "# Load the English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Read the saved text file\n",
    "with open(output_text, 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Tokenize using spaCy\n",
    "doc = nlp(text)\n",
    "tokens = [token.text for token in doc]\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract text using Tesseract library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "# Open the image\n",
    "img = Image.open(original_image)\n",
    "\n",
    "custom_config = r'--psm 11'\n",
    "\n",
    "# Extract text with the specified page segmentation mode\n",
    "extracted_text = pytesseract.image_to_string(img, config=custom_config)\n",
    "\n",
    "# Print the extracted text\n",
    "print(extracted_text)\n",
    "\n",
    "# Save the extracted text to a file\n",
    "output_text = 'output_text/tesseract_output_text_1.txt'\n",
    "\n",
    "with open(output_text, 'w') as file:\n",
    "    file.write(extracted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region-based Extraction using Contour Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "image = cv2.imread(original_image)\n",
    "\n",
    "# Convert to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply adaptive thresholding to binarize the image\n",
    "thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "\n",
    "# Use OpenCV to detect contours (boxes/fields in the form)\n",
    "contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Loop through contours to extract text from specific regions\n",
    "for contour in contours:\n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "    \n",
    "    # Crop the region of interest (ROI)\n",
    "    roi = image[y:y+h, x:x+w]\n",
    "    \n",
    "    # Convert the ROI to a PIL image for Tesseract\n",
    "    roi_pil = Image.fromarray(roi)\n",
    "\n",
    "    # Use Tesseract with appropriate PSM\n",
    "    custom_config = r'--psm 11'\n",
    "    text = pytesseract.image_to_string(roi_pil, config=custom_config)\n",
    "\n",
    "    # Print or save extracted text from each ROI\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region-based Extraction + True Label Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the form image\n",
    "image = cv2.imread(original_image)\n",
    "\n",
    "# Convert to grayscale for better thresholding\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply thresholding to enhance the detection of black boxes\n",
    "thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)\n",
    "\n",
    "# Detect edges to find the contours of the form boxes\n",
    "edges = cv2.Canny(thresh, 30, 150)\n",
    "\n",
    "# Find contours in the edges image\n",
    "contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Define a list of labels corresponding to the form fields (manually)\n",
    "field_labels = [\"Date\", \"City\", \"State\", \"ZIP\"]\n",
    "\n",
    "# Initialize a dictionary to hold the extracted text and the associated labels\n",
    "extracted_data = {}\n",
    "\n",
    "# Sort contours by their position (optional, depends on the form structure)\n",
    "contours = sorted(contours, key=lambda x: cv2.boundingRect(x)[1])  # Sort by the y-coordinate of each contour\n",
    "\n",
    "# Loop through each contour and filter by size (exclude very large and very small contours)\n",
    "for i, contour in enumerate(contours):\n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "    \n",
    "    # Filter out contours that are too large or too small to be boxes\n",
    "    if w < 50 or h < 20 or w > 500 or h > 200:  # Adjust these values based on your form's structure\n",
    "        continue\n",
    "\n",
    "    # Crop the region of interest (ROI) from the image\n",
    "    roi = image[y:y+h, x:x+w]\n",
    "    \n",
    "    # Convert the ROI to a PIL image for Tesseract\n",
    "    roi_pil = Image.fromarray(roi)\n",
    "    \n",
    "    # Use Tesseract to extract text from the ROI\n",
    "    custom_config = r'--psm 11'\n",
    "    extracted_text = pytesseract.image_to_string(roi_pil, config=custom_config)\n",
    "    \n",
    "    # Clean up the extracted text (optional)\n",
    "    extracted_text = extracted_text.strip()\n",
    "\n",
    "    # Map the extracted text to the corresponding label (assuming the contours align with the labels)\n",
    "    label = field_labels[i % len(field_labels)]  # Avoid out-of-range errors\n",
    "    extracted_data[label] = extracted_text\n",
    "\n",
    "    # Optional: Draw the bounding box on the image for debugging\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "# Function to display the image using Matplotlib\n",
    "def show_image(img, title=\"Image\"):\n",
    "    # Convert BGR (used by OpenCV) to RGB (used by Matplotlib)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')  # Hide axis\n",
    "    plt.show()\n",
    "\n",
    "# Show the final image with detected regions (boxes)\n",
    "show_image(image, \"Detected Regions\")\n",
    "\n",
    "# Print the extracted data (you can also save this to a text file)\n",
    "print(extracted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region-based Text Extraction + Provided Co-ordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Function to load true labels for a specific image\n",
    "def load_true_labels(image_id, labels_file='true_labels.json'):\n",
    "    with open(labels_file) as f:\n",
    "        true_labels_data = json.load(f)\n",
    "    return true_labels_data.get(image_id, {})\n",
    "\n",
    "# Load the form image\n",
    "image = cv2.imread(original_image)\n",
    "image_id = os.path.splitext(os.path.basename(original_image))[0]\n",
    "\n",
    "# Convert to grayscale for better thresholding\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Load the coordinates and labels from the JSON file\n",
    "with open('box_coordinates.json') as f:\n",
    "    box_data = json.load(f)\n",
    "\n",
    "# Load the true labels for this specific image\n",
    "true_labels = load_true_labels(image_id)\n",
    "\n",
    "# Initialize a dictionary to hold the extracted text\n",
    "extracted_data = {}\n",
    "\n",
    "# Loop through each box and extract the text\n",
    "for box in box_data:\n",
    "    label = box['label']\n",
    "    x, y, w, h = box['x'], box['y'], box['w'], box['h']\n",
    "    true_label = true_labels.get(label, \"N/A\")\n",
    "    \n",
    "    # Crop the region of interest (ROI) from the image\n",
    "    roi = image[y:y+h, x:x+w]\n",
    "    \n",
    "    # Convert the ROI to a PIL image for Tesseract\n",
    "    roi_pil = Image.fromarray(roi)\n",
    "    \n",
    "    # Use Tesseract to extract text from the ROI\n",
    "    custom_config = r'--psm 6'\n",
    "    extracted_text = pytesseract.image_to_string(roi_pil, config=custom_config)\n",
    "    \n",
    "    # Clean up the extracted text (optional)\n",
    "    extracted_text = extracted_text.strip()\n",
    "    \n",
    "    # Map the extracted text to the corresponding label and true label\n",
    "    extracted_data[label] = {\n",
    "        'extracted_text': extracted_text,\n",
    "        'true_label': true_label\n",
    "    }\n",
    "\n",
    "    # Optional: Draw the bounding box on the image for visualization\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "# Show the final image with bounding boxes\n",
    "show_image(image, \"Detected Regions\")\n",
    "\n",
    "# Print the extracted data\n",
    "print(extracted_data)\n",
    "\n",
    "# Save the extracted data as a JSON file\n",
    "output_file = f\"output_text/{image_id}_extracted.json\"\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(extracted_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Extraction with pre-trained TrOCR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kkoma\\Documents\\GitHub\\nlp_handwritten_forms\\myenv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "VisionEncoderDecoderModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): TrOCRForCausalLM(\n",
       "    (model): TrOCRDecoderWrapper(\n",
       "      (decoder): TrOCRDecoder(\n",
       "        (embed_tokens): TrOCRScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 1024)\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x TrOCRDecoderLayer(\n",
       "            (self_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (output_projection): Linear(in_features=1024, out_features=50265, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pre-trained model for handwritten text\n",
    "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\n",
    "model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Text: 0 1\n"
     ]
    }
   ],
   "source": [
    "# Function to perform inference on an image\n",
    "def predict_text(image_path):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"L\")  # Convert to grayscale\n",
    "    image = image.resize((image.width // 2, image.height // 2))\n",
    "\n",
    "    # Add a new dimension to convert from (H, W) to (1, H, W)\n",
    "    image = image.convert(\"RGB\")  # Convert back to RGB for the model\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "    # Perform inference without tracking gradients\n",
    "    with torch.no_grad():\n",
    "        # Generate the text using max_new_tokens for better control over output length\n",
    "        generated_ids = model.generate(inputs, max_new_tokens=100)  # Adjust max_new_tokens as needed\n",
    "\n",
    "    # Decode the generated text, skipping special tokens\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "predicted_text = predict_text(original_image)\n",
    "print(\"Predicted Text:\", predicted_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
