{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR for Handwritten Forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select image to extract text from\n",
    "original_image = 'images/image_1.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OpenCV library\n",
    "import cv2\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(original_image)\n",
    "\n",
    "# Convert to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply adaptive thresholding\n",
    "thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                               cv2.THRESH_BINARY, 11, 2)\n",
    "\n",
    "# Save the processed image (optional)\n",
    "processed_image = 'processed_images/processed_image_1.jpg'\n",
    "cv2.imwrite(processed_image, thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Text using EasyOCR library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import EasyOCR library\n",
    "import easyocr\n",
    "\n",
    "# Initialize reader object\n",
    "reader = easyocr.Reader(['en'])\n",
    "\n",
    "# Extract text from original image\n",
    "result = reader.readtext(original_image, detail=0)\n",
    "\n",
    "# Join the extracted text into a single string\n",
    "extracted_text = '\\n'.join(result)\n",
    "\n",
    "# Check output\n",
    "print(extracted_text)\n",
    "\n",
    "# Save the extracted text to a file\n",
    "output_text = 'output_text/easyocr_output_text_1.txt'\n",
    "with open(output_text, 'w') as file:\n",
    "    file.write(extracted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SpaCy library\n",
    "import spacy\n",
    "\n",
    "# Load the English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Read the saved text file\n",
    "with open(output_text, 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Tokenize using spaCy\n",
    "doc = nlp(text)\n",
    "tokens = [token.text for token in doc]\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract text using Tesseract library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "# Open the image\n",
    "img = Image.open(original_image)\n",
    "\n",
    "custom_config = r'--psm 11'\n",
    "\n",
    "# Extract text with the specified page segmentation mode\n",
    "extracted_text = pytesseract.image_to_string(img, config=custom_config)\n",
    "\n",
    "# Print the extracted text\n",
    "print(extracted_text)\n",
    "\n",
    "# Save the extracted text to a file\n",
    "output_text = 'output_text/tesseract_output_text_1.txt'\n",
    "\n",
    "with open(output_text, 'w') as file:\n",
    "    file.write(extracted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region-based Extraction using Contour Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "image = cv2.imread(original_image)\n",
    "\n",
    "# Convert to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply adaptive thresholding to binarize the image\n",
    "thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "\n",
    "# Use OpenCV to detect contours (boxes/fields in the form)\n",
    "contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Loop through contours to extract text from specific regions\n",
    "for contour in contours:\n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "    \n",
    "    # Crop the region of interest (ROI)\n",
    "    roi = image[y:y+h, x:x+w]\n",
    "    \n",
    "    # Convert the ROI to a PIL image for Tesseract\n",
    "    roi_pil = Image.fromarray(roi)\n",
    "\n",
    "    # Use Tesseract with appropriate PSM\n",
    "    custom_config = r'--psm 11'\n",
    "    text = pytesseract.image_to_string(roi_pil, config=custom_config)\n",
    "\n",
    "    # Print or save extracted text from each ROI\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region-based Extraction + True Label Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the form image\n",
    "image = cv2.imread(original_image)\n",
    "\n",
    "# Convert to grayscale for better thresholding\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply thresholding to enhance the detection of black boxes\n",
    "thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)\n",
    "\n",
    "# Detect edges to find the contours of the form boxes\n",
    "edges = cv2.Canny(thresh, 30, 150)\n",
    "\n",
    "# Find contours in the edges image\n",
    "contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Define a list of labels corresponding to the form fields (manually)\n",
    "field_labels = [\"Date\", \"City\", \"State\", \"ZIP\"]\n",
    "\n",
    "# Initialize a dictionary to hold the extracted text and the associated labels\n",
    "extracted_data = {}\n",
    "\n",
    "# Sort contours by their position (optional, depends on the form structure)\n",
    "contours = sorted(contours, key=lambda x: cv2.boundingRect(x)[1])  # Sort by the y-coordinate of each contour\n",
    "\n",
    "# Loop through each contour and filter by size (exclude very large and very small contours)\n",
    "for i, contour in enumerate(contours):\n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "    \n",
    "    # Filter out contours that are too large or too small to be boxes\n",
    "    if w < 50 or h < 20 or w > 500 or h > 200:  # Adjust these values based on your form's structure\n",
    "        continue\n",
    "\n",
    "    # Crop the region of interest (ROI) from the image\n",
    "    roi = image[y:y+h, x:x+w]\n",
    "    \n",
    "    # Convert the ROI to a PIL image for Tesseract\n",
    "    roi_pil = Image.fromarray(roi)\n",
    "    \n",
    "    # Use Tesseract to extract text from the ROI\n",
    "    custom_config = r'--psm 11'\n",
    "    extracted_text = pytesseract.image_to_string(roi_pil, config=custom_config)\n",
    "    \n",
    "    # Clean up the extracted text (optional)\n",
    "    extracted_text = extracted_text.strip()\n",
    "\n",
    "    # Map the extracted text to the corresponding label (assuming the contours align with the labels)\n",
    "    label = field_labels[i % len(field_labels)]  # Avoid out-of-range errors\n",
    "    extracted_data[label] = extracted_text\n",
    "\n",
    "    # Optional: Draw the bounding box on the image for debugging\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "# Function to display the image using Matplotlib\n",
    "def show_image(img, title=\"Image\"):\n",
    "    # Convert BGR (used by OpenCV) to RGB (used by Matplotlib)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')  # Hide axis\n",
    "    plt.show()\n",
    "\n",
    "# Show the final image with detected regions (boxes)\n",
    "show_image(image, \"Detected Regions\")\n",
    "\n",
    "# Print the extracted data (you can also save this to a text file)\n",
    "print(extracted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region-based Text Extraction + Provided Co-ordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Function to load true labels for a specific image\n",
    "def load_true_labels(image_id, labels_file='true_labels.json'):\n",
    "    with open(labels_file) as f:\n",
    "        true_labels_data = json.load(f)\n",
    "    return true_labels_data.get(image_id, {})\n",
    "\n",
    "# Load the form image\n",
    "image = cv2.imread(original_image)\n",
    "image_id = os.path.splitext(os.path.basename(original_image))[0]\n",
    "\n",
    "# Convert to grayscale for better thresholding\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Load the coordinates and labels from the JSON file\n",
    "with open('box_coordinates.json') as f:\n",
    "    box_data = json.load(f)\n",
    "\n",
    "# Load the true labels for this specific image\n",
    "true_labels = load_true_labels(image_id)\n",
    "\n",
    "# Initialize a dictionary to hold the extracted text\n",
    "extracted_data = {}\n",
    "\n",
    "# Loop through each box and extract the text\n",
    "for box in box_data:\n",
    "    label = box['label']\n",
    "    x, y, w, h = box['x'], box['y'], box['w'], box['h']\n",
    "    true_label = true_labels.get(label, \"N/A\")\n",
    "    \n",
    "    # Crop the region of interest (ROI) from the image\n",
    "    roi = image[y:y+h, x:x+w]\n",
    "    \n",
    "    # Convert the ROI to a PIL image for Tesseract\n",
    "    roi_pil = Image.fromarray(roi)\n",
    "    \n",
    "    # Use Tesseract to extract text from the ROI\n",
    "    custom_config = r'--psm 6'\n",
    "    extracted_text = pytesseract.image_to_string(roi_pil, config=custom_config)\n",
    "    \n",
    "    # Clean up the extracted text (optional)\n",
    "    extracted_text = extracted_text.strip()\n",
    "    \n",
    "    # Map the extracted text to the corresponding label and true label\n",
    "    extracted_data[label] = {\n",
    "        'extracted_text': extracted_text,\n",
    "        'true_label': true_label\n",
    "    }\n",
    "\n",
    "    # Optional: Draw the bounding box on the image for visualization\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "# Show the final image with bounding boxes\n",
    "show_image(image, \"Detected Regions\")\n",
    "\n",
    "# Print the extracted data\n",
    "print(extracted_data)\n",
    "\n",
    "# Save the extracted data as a JSON file\n",
    "output_file = f\"output_text/{image_id}_extracted.json\"\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(extracted_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Extraction with TrOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import torch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
